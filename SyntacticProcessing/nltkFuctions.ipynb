{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleSentence = \"Bangalore is the capital of karnataka\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsInSentence = nltk.word_tokenize(simpleSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bangalore', 'is', 'the', 'capital', 'of', 'karnataka']\n"
     ]
    }
   ],
   "source": [
    "print(wordsInSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "partsOfSpeechTags = nltk.pos_tag(wordsInSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bangalore', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('capital', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('karnataka', 'NN')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partsOfSpeechTags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing your Own Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learnDefaultTagger(simpleSentence):\n",
    "    wordsInSentence = nltk.word_tokenize(simpleSentence)\n",
    "    tagger = nltk.DefaultTagger(\"NN\")\n",
    "    posEnabledTags = tagger.tag(wordsInSentence)\n",
    "    print(posEnabledTags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learnRETagger(simpleSentence):\n",
    "    customPatterns = [\n",
    "        (r'.*ing$','ADJECTIVE'),\n",
    "        (r'.*ly$','ADVERB'),\n",
    "        (r'.*ion$','NOUN'),\n",
    "        (r'.*ate |.*en|is$','VERB'),\n",
    "        (r'.^an$$','INDEFINITE-ARTICLE'),\n",
    "        (r'.^(with|on|at)$','PREPOSITION'),\n",
    "        (r'.^\\-?[0-9]+(\\.[0-9]+)$','NUMBER'),\n",
    "        (r'.*$',None)        \n",
    "    ]\n",
    "    wordsInSentence = nltk.word_tokenize(simpleSentence)\n",
    "    tagger = nltk.RegexpTagger(customPatterns)\n",
    "    posEnabledTags = tagger.tag(wordsInSentence)\n",
    "    print(posEnabledTags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learnLookupTagger(simpleSentence):\n",
    "    mapping  = {\n",
    "        '.':'.','place':'NN','on':'IN','earth':'NN',\n",
    "        'Mysore':'NNP','is':'VBZ','an':'DT','amazing':'JJ'        \n",
    "    }\n",
    "    wordsInSentence = nltk.word_tokenize(simpleSentence)\n",
    "    tagger = nltk.UnigramTagger(model=mapping)\n",
    "    posEnabledTags = tagger.tag(wordsInSentence)\n",
    "    print(posEnabledTags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Mysore', 'NN'), ('is', 'NN'), ('an', 'NN'), ('amazing', 'NN'), ('place', 'NN'), ('on', 'NN'), ('earth', 'NN'), (',', 'NN'), ('I', 'NN'), ('have', 'NN'), ('visited', 'NN'), ('Mysore', 'NN'), ('10', 'NN'), ('times', 'NN'), ('.', 'NN')]\n",
      "\n",
      "\n",
      "[('Mysore', None), ('is', 'VERB'), ('an', None), ('amazing', 'ADJECTIVE'), ('place', None), ('on', None), ('earth', None), (',', None), ('I', None), ('have', None), ('visited', None), ('Mysore', None), ('10', None), ('times', None), ('.', None)]\n",
      "\n",
      "\n",
      "[('Mysore', 'NNP'), ('is', 'VBZ'), ('an', 'DT'), ('amazing', 'JJ'), ('place', 'NN'), ('on', 'IN'), ('earth', 'NN'), (',', None), ('I', None), ('have', None), ('visited', None), ('Mysore', 'NNP'), ('10', None), ('times', None), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    testSentence = \"Mysore is an amazing place on earth, I have visited Mysore 10 times.\"\n",
    "    learnDefaultTagger(testSentence)\n",
    "    print(\"\\n\")\n",
    "    learnRETagger(testSentence)\n",
    "    print(\"\\n\")\n",
    "    learnLookupTagger(testSentence)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training your own Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle,nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleData():\n",
    "    return [\n",
    "        \"Bangalore is the capital of Karnataka.\",\n",
    "        \"Steve Jobs was the CEO of Apple\",\n",
    "        \"iPhone was Invented by Apple\",\n",
    "        \"Books can be purchased in Market Iphone\"\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDictionary():\n",
    "    dictionary = {}\n",
    "    for sent in sampleData():\n",
    "        partsOfSpeechTags = nltk.pos_tag(nltk.word_tokenize(sent))\n",
    "        for tag in partsOfSpeechTags:\n",
    "            value = tag[0]\n",
    "            pos = tag[1]\n",
    "            dictionary[value] = pos\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveMyTagger(tagger,fileName):\n",
    "    fileHandle = open(fileName,\"wb\")\n",
    "    pickle.dump(tagger,fileHandle)\n",
    "    fileHandle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveMyTraining(fileName):\n",
    "    tagger = nltk.UnigramTagger(model=buildDictionary())\n",
    "    saveMyTagger(tagger,fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadMyTagger(fileName):\n",
    "    return pickle.load(open(fileName,\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Iphone is purchased by Stevejobs in Bangalore Market\"\n",
    "fileName = \"myTagger.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveMyTraining(fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "myTagger = loadMyTagger(fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Iphone', 'NNP'), ('is', 'VBZ'), ('purchased', 'VBN'), ('by', 'IN'), ('Stevejobs', None), ('in', 'IN'), ('Bangalore', 'NNP'), ('Market', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "print(myTagger.tag(nltk.word_tokenize(sentence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning to write your own grammar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.generate import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "del productions\n",
    "productions = [\n",
    "    \"ROOT -> WORD\",\n",
    "    \"WORD -> ' '\",\n",
    "    \"WORD -> NUMBER LETTER\",\n",
    "    \"WORD -> LETTER NUMBER\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = list(string.digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for digit in digits[:4]:\n",
    "    productions.append(\"NUMBER  -> '{w}'\".format(w=digit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = \"' | '\".join(list(string.ascii_lowercase)[:4])\n",
    "productions.append(\"LETTER  -> '{w}'\".format(w=letters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"a' | 'b' | 'c' | 'd\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammarString = \"\\n\".join(productions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = nltk.CFG.fromstring(grammarString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar with 12 productions (start state = ROOT)\n",
      "    ROOT -> WORD\n",
      "    WORD -> ' '\n",
      "    WORD -> NUMBER LETTER\n",
      "    WORD -> LETTER NUMBER\n",
      "    NUMBER -> '0'\n",
      "    NUMBER -> '1'\n",
      "    NUMBER -> '2'\n",
      "    NUMBER -> '3'\n",
      "    LETTER -> 'a'\n",
      "    LETTER -> 'b'\n",
      "    LETTER -> 'c'\n",
      "    LETTER -> 'd'\n"
     ]
    }
   ],
   "source": [
    "print(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Word : , Size : 0\n",
      "Generated Word : 0a, Size : 2\n",
      "Generated Word : 0b, Size : 2\n",
      "Generated Word : 0c, Size : 2\n",
      "Generated Word : 0d, Size : 2\n",
      "Generated Word : 1a, Size : 2\n"
     ]
    }
   ],
   "source": [
    "for sentence in generate(grammar, n=6,depth=6):\n",
    "    #print(sentence)\n",
    "    palindrome = \"\".join(sentence).replace(\" \",\"\")\n",
    "    print(\"Generated Word : {}, Size : {}\".format(palindrome,len(palindrome)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CFG is a special type of CFG in which the sum of all the probabilities for the non terminal tokens should be equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.generate import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "productions  = [\n",
    "    \"ROOT -> WORD [1.0]\",\n",
    "    \"WORD -> P1 [0.25]\",\n",
    "    \"WORD -> P1 P2 [0.25]\",\n",
    "    \"WORD -> P1 P2 P3 [0.25]\",\n",
    "    \"WORD -> P1 P2 P3 P4 [0.25]\",\n",
    "    \"P1 -> 'A' [1.0]\",\n",
    "    \"P2 -> 'B' [0.5]\",\n",
    "    \"P2 -> 'C' [0.5]\",\n",
    "    \"P3 -> 'D' [0.3]\",\n",
    "    \"P3 -> 'E' [0.3]\",\n",
    "    \"P3 -> 'F' [0.4]\",\n",
    "    \"P4 -> 'G' [0.9]\",\n",
    "    \"P4 -> 'H' [0.1]\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammarString = \"\\n\".join(productions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = nltk.PCFG.fromstring(grammarString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar with 13 productions (start state = ROOT)\n",
      "    ROOT -> WORD [1.0]\n",
      "    WORD -> P1 [0.25]\n",
      "    WORD -> P1 P2 [0.25]\n",
      "    WORD -> P1 P2 P3 [0.25]\n",
      "    WORD -> P1 P2 P3 P4 [0.25]\n",
      "    P1 -> 'A' [1.0]\n",
      "    P2 -> 'B' [0.5]\n",
      "    P2 -> 'C' [0.5]\n",
      "    P3 -> 'D' [0.3]\n",
      "    P3 -> 'E' [0.3]\n",
      "    P3 -> 'F' [0.4]\n",
      "    P4 -> 'G' [0.9]\n",
      "    P4 -> 'H' [0.1]\n"
     ]
    }
   ],
   "source": [
    "print(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String : A,Size : 1\n",
      "String : AB,Size : 2\n",
      "String : AC,Size : 2\n",
      "String : ABD,Size : 3\n",
      "String : ABE,Size : 3\n",
      "String : ABF,Size : 3\n",
      "String : ACD,Size : 3\n",
      "String : ACE,Size : 3\n",
      "String : ACF,Size : 3\n",
      "String : ABDG,Size : 4\n",
      "String : ABDH,Size : 4\n",
      "String : ABEG,Size : 4\n",
      "String : ABEH,Size : 4\n",
      "String : ABFG,Size : 4\n",
      "String : ABFH,Size : 4\n",
      "String : ACDG,Size : 4\n",
      "String : ACDH,Size : 4\n",
      "String : ACEG,Size : 4\n",
      "String : ACEH,Size : 4\n",
      "String : ACFG,Size : 4\n",
      "String : ACFH,Size : 4\n"
     ]
    }
   ],
   "source": [
    "for sentence in generate(grammar, n = 100, depth=5):\n",
    "    palindrome = \"\".join(sentence).replace(\" \",\"\")\n",
    "    print(\"String : {},Size : {}\".format(palindrome,len(palindrome)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Recursive CFGs are a special types of CFGs, where the tokens on the left hand side are present on the right hand side of a production rule. Palindromes are the best examples of recursive CFG.\n",
    "\n",
    "#### Writing a recursive CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "productions = [\n",
    "    \"ROOT -> WORD\",\n",
    "    \"WORD -> ' '\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets = list(string.digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alphabet in alphabets:\n",
    "    productions.append(\"WORD -> '{w}' WORD '{w}'\".format(w=alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammarString = \"\\n\".join(productions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = nltk.CFG.fromstring(grammarString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar with 12 productions (start state = ROOT)\n",
      "    ROOT -> WORD\n",
      "    WORD -> ' '\n",
      "    WORD -> '0' WORD '0'\n",
      "    WORD -> '1' WORD '1'\n",
      "    WORD -> '2' WORD '2'\n",
      "    WORD -> '3' WORD '3'\n",
      "    WORD -> '4' WORD '4'\n",
      "    WORD -> '5' WORD '5'\n",
      "    WORD -> '6' WORD '6'\n",
      "    WORD -> '7' WORD '7'\n",
      "    WORD -> '8' WORD '8'\n",
      "    WORD -> '9' WORD '9'\n"
     ]
    }
   ],
   "source": [
    "print(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palindrome : ,Size : 0\n",
      "Palindrome : 00,Size : 2\n",
      "Palindrome : 0000,Size : 4\n",
      "Palindrome : 0110,Size : 4\n",
      "Palindrome : 0220,Size : 4\n",
      "Palindrome : 0330,Size : 4\n",
      "Palindrome : 0440,Size : 4\n",
      "Palindrome : 0550,Size : 4\n",
      "Palindrome : 0660,Size : 4\n",
      "Palindrome : 0770,Size : 4\n"
     ]
    }
   ],
   "source": [
    "for sentence in generate(grammar, n = 10, depth=5):\n",
    "    palindrome = \"\".join(sentence).replace(\" \",\"\")\n",
    "    print(\"Palindrome : {},Size : {}\".format(palindrome,len(palindrome)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section taught us that part of speech tagging forms the basis of any further syntactic analysis, and grammars can be formed and deformed using part of speech tags and chunks. We have learned to use and write our own POS taggers and grammars, awesome. In the next section we'll see chunking, sentence parse, and dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Lalbagh Botanical Gradens isa well known botanical garden in Bengaluru, India.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lalbagh Botanical Gradens isa well known botanical garden in Bengaluru, India.']\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Lalbagh/NNP)\n",
      "  (PERSON Botanical/NNP Gradens/NNP)\n",
      "  isa/RB\n",
      "  well/RB\n",
      "  known/VBN\n",
      "  botanical/JJ\n",
      "  garden/NN\n",
      "  in/IN\n",
      "  (GPE Bengaluru/NNP)\n",
      "  ,/,\n",
      "  (GPE India/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    tags = nltk.pos_tag(words)\n",
    "    chunks = nltk.ne_chunk(tags)\n",
    "    print(chunks)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write own simple chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Ravi is the CEO of a company. He is very powerful public speaker also.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = \"\\n\".join([\n",
    "    'NP: {<DT>*<NNP>}',\n",
    "    'NP: {<JJ>*<NN>}',\n",
    "    'NP: {<NNP>+}',\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Ravi/NNP)\n",
      "  is/VBZ\n",
      "  (NP the/DT CEO/NNP)\n",
      "  of/IN\n",
      "  a/DT\n",
      "  (NP company/NN)\n",
      "  ./.)\n",
      "(S\n",
      "  He/PRP\n",
      "  is/VBZ\n",
      "  very/RB\n",
      "  (NP powerful/JJ public/JJ speaker/NN)\n",
      "  also/RB\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    tags = nltk.pos_tag(words)\n",
    "    chunkparser = nltk.RegexpParser(grammar)\n",
    "    result = chunkparser.parse(tags)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2000\n",
    "from nltk.corpus import treebank_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mySimpleChunker():\n",
    "    grammar = 'NP: {<NNP>+}'\n",
    "    return nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_nothing(data):\n",
    "    cp = nltk.RegexpParser(\"\")\n",
    "    print(cp.evaluate(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mysimplechunker(data):\n",
    "    schunker = mySimpleChunker()\n",
    "    print(schunker.evaluate(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    conll2000.chunked_sents('test.txt',chunk_types=['NP']),\n",
    "    treebank_chunk.chunked_sents()\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  38.6%%\n",
      "    Precision:      0.0%%\n",
      "    Recall:         0.0%%\n",
      "    F-Measure:      0.0%%\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  48.2%%\n",
      "    Precision:     71.1%%\n",
      "    Recall:        17.2%%\n",
      "    F-Measure:     27.7%%\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  45.0%%\n",
      "    Precision:      0.0%%\n",
      "    Recall:         0.0%%\n",
      "    F-Measure:      0.0%%\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  50.7%%\n",
      "    Precision:     51.9%%\n",
      "    Recall:         8.8%%\n",
      "    F-Measure:     15.1%%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\babas\\Anaconda3\\envs\\NLP_3.6\\lib\\site-packages\\nltk\\tokenize\\regexp.py:130: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return [tok for tok in self._regexp.split(text) if tok]\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    test_nothing(dataset[:50])\n",
    "    test_mysimplechunker(dataset[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RDParserExample(grammar,textlist):\n",
    "    parser = nltk.parse.RecursiveDescentParser(grammar)\n",
    "    for text in textlist:\n",
    "        sentence = nltk.word_tokenize(text)\n",
    "        for tree in parser.parse(sentence):\n",
    "            print(tree)\n",
    "            tree.draw()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "NP -> NNP VBZ\n",
    "VP -> IN NNP | DT NN IN NNP\n",
    "NNP -> 'Tajmahal' | 'Agra' | 'Bangalore' | 'Karnataka'\n",
    "VBZ -> 'is'\n",
    "IN -> 'in' | 'of'\n",
    "DT -> 'the'\n",
    "NN -> 'capital'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "    \"Tajmahal is in Agra\",\n",
    "    \"Bangalore is the capital of Karnataka\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP (NNP Tajmahal) (VBZ is)) (VP (IN in) (NNP Agra)))\n",
      "(S\n",
      "  (NP (NNP Bangalore) (VBZ is))\n",
      "  (VP (DT the) (NN capital) (IN of) (NNP Karnataka)))\n"
     ]
    }
   ],
   "source": [
    "RDParserExample(grammar,text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shift Reduce parser\n",
    "\n",
    "Shift reduce parsers are special types of parsers that parse the input text from left to right on a single line sentences and top to bottom on multi-line sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def SRParserExample(grammar, textlist):\n",
    "    parser = nltk.parse.ShiftReduceParser(grammar)\n",
    "    for text in textlist:\n",
    "        sentence = nltk.word_tokenize(text)\n",
    "        for tree in parser.parse(sentence):\n",
    "            print(tree)\n",
    "            tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "    \"Tajmahal is in Agra\",\n",
    "    \"Bangalore is the capital of Karnataka\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "NP -> NNP VBZ\n",
    "VP -> IN NNP | DT NN IN NNP\n",
    "NNP -> 'Tajmahal' | 'Agra' | 'Bangalore' | 'Karnataka'\n",
    "VBZ -> 'is'\n",
    "IN -> 'in' | 'of'\n",
    "DT -> 'the'\n",
    "NN -> 'capital'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP (NNP Tajmahal) (VBZ is)) (VP (IN in) (NNP Agra)))\n"
     ]
    }
   ],
   "source": [
    "SRParserExample(grammar, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing dependency grammar and projective dependencyÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = nltk.grammar.DependencyGrammar.fromstring(\"\"\"\n",
    "'savings' -> 'small'\n",
    "'yield' -> 'savings'\n",
    "'gains' -> 'large'\n",
    "'yield' -> 'gains'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'small savings yield large gains'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = nltk.parse.ProjectiveDependencyParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(yield (savings small) (gains large))\n"
     ]
    }
   ],
   "source": [
    "for t in sorted(dp.parse(sentence.split())):\n",
    "    print(t)\n",
    "    t.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing a chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.grammar import CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.chart import ChartParser, BU_LC_STRATEGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = CFG.fromstring(\"\"\"\n",
    "S -> T1 T4\n",
    "T1 -> NNP VBZ\n",
    "T2 -> DT NN\n",
    "T3 -> IN NNP\n",
    "T4 -> T3 | T2 T3\n",
    "NNP -> 'Tajmahal' | 'Agra' | 'Bangalore' | 'Karnataka'\n",
    "VBZ -> 'is'\n",
    "IN -> 'in' | 'of'\n",
    "DT -> 'the'\n",
    "NN -> 'capital'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = ChartParser(grammar, BU_LC_STRATEGY, trace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Bangalore is the capital of Karnataka\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|.Bangal.  is  . the  .capita.  of  .Karnat.|\n",
      "|[------]      .      .      .      .      .| [0:1] 'Bangalore'\n",
      "|.      [------]      .      .      .      .| [1:2] 'is'\n",
      "|.      .      [------]      .      .      .| [2:3] 'the'\n",
      "|.      .      .      [------]      .      .| [3:4] 'capital'\n",
      "|.      .      .      .      [------]      .| [4:5] 'of'\n",
      "|.      .      .      .      .      [------]| [5:6] 'Karnataka'\n",
      "|[------]      .      .      .      .      .| [0:1] NNP -> 'Bangalore' *\n",
      "|[------>      .      .      .      .      .| [0:1] T1 -> NNP * VBZ\n",
      "|.      [------]      .      .      .      .| [1:2] VBZ -> 'is' *\n",
      "|[-------------]      .      .      .      .| [0:2] T1 -> NNP VBZ *\n",
      "|[------------->      .      .      .      .| [0:2] S  -> T1 * T4\n",
      "|.      .      [------]      .      .      .| [2:3] DT -> 'the' *\n",
      "|.      .      [------>      .      .      .| [2:3] T2 -> DT * NN\n",
      "|.      .      .      [------]      .      .| [3:4] NN -> 'capital' *\n",
      "|.      .      [-------------]      .      .| [2:4] T2 -> DT NN *\n",
      "|.      .      [------------->      .      .| [2:4] T4 -> T2 * T3\n",
      "|.      .      .      .      [------]      .| [4:5] IN -> 'of' *\n",
      "|.      .      .      .      [------>      .| [4:5] T3 -> IN * NNP\n",
      "|.      .      .      .      .      [------]| [5:6] NNP -> 'Karnataka' *\n",
      "|.      .      .      .      .      [------>| [5:6] T1 -> NNP * VBZ\n",
      "|.      .      .      .      [-------------]| [4:6] T3 -> IN NNP *\n",
      "|.      .      .      .      [-------------]| [4:6] T4 -> T3 *\n",
      "|.      .      [---------------------------]| [2:6] T4 -> T2 T3 *\n",
      "|[=========================================]| [0:6] S  -> T1 T4 *\n"
     ]
    }
   ],
   "source": [
    "chart = cp.chart_parse(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "parses = list(chart.parses(grammar.start()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Edges : 24\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Edges :\", len(chart.edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (T1 (NNP Bangalore) (VBZ is))\n",
      "  (T4 (T2 (DT the) (NN capital)) (T3 (IN of) (NNP Karnataka))))\n"
     ]
    }
   ],
   "source": [
    "for tree in parses: print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
